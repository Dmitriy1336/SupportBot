import string
import numpy as np
import spacy
from aiogram import Router, F
from sentence_transformers import SentenceTransformer, util
from concurrent.futures import ThreadPoolExecutor
import asyncio
import os
from aiogram.types import Message
from aiogram.fsm.state import StatesGroup, State
from aiogram.fsm.context import FSMContext
import logging
from aiogram.utils.keyboard import InlineKeyboardBuilder
from aiogram.types import CallbackQuery, InlineKeyboardButton
# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)

router = Router()

class ChatGPT(StatesGroup):
    Set_Chat_GPT = State()

# –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –æ—Ç–≤–µ—Ç–æ–≤ –∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
original_answers = []
answer_embeddings = []

MAX_MESSAGE_LENGTH = 4096


# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ spaCy –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞
nlp = spacy.load("ru_core_news_sm")

# –°–æ–∑–¥–∞–Ω–∏–µ –ø—É–ª–∞ –ø–æ—Ç–æ–∫–æ–≤ –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è —Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ
executor = ThreadPoolExecutor()

model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')

def count_word_matches(query, response):
    query_words = set(query.split())
    response_words = set(response.split())
    return len(query_words.intersection(response_words))

def split_message(message):
    """–†–∞–∑–±–∏–≤–∞–µ—Ç –¥–ª–∏–Ω–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –Ω–∞ —á–∞—Å—Ç–∏, –Ω–µ –ø—Ä–µ–≤—ã—à–∞—é—â–∏–µ –º–∞–∫—Å–∏–º–∞–ª—å–Ω—É—é –¥–ª–∏–Ω—É"""
    return [message[i:i + MAX_MESSAGE_LENGTH] for i in range(0, len(message), MAX_MESSAGE_LENGTH)]

def preprocess_text_spacy(text):
    try:
        text = text.lower()
        text = text.translate(str.maketrans('', '', string.punctuation))
        doc = nlp(text)
        words = [token.lemma_ for token in doc if not token.is_stop and not token.is_punct]
        processed_text = ' '.join(words)
        return processed_text
    except Exception as e:
        logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–µ —Ç–µ–∫—Å—Ç–∞: {e}")
        return text  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –∏—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏

async def preprocess_text_spacy_async(text):
    loop = asyncio.get_event_loop()
    return await loop.run_in_executor(executor, preprocess_text_spacy, text)

def read_data_with_embeddings(file_path, model, batch_size=10):
    try:
        if not os.path.exists(file_path):
            logging.error(f"–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {file_path}")
            return [], np.array([])

        with open(file_path, 'r', encoding='utf-8') as file:
            lines = file.readlines()

        logging.info(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫ –≤ —Ñ–∞–π–ª–µ: {len(lines)}")

        original_answers = []
        all_embeddings = []

        for i in range(0, len(lines), batch_size):
            batch_lines = lines[i:i + batch_size]
            batch_answers = [line.strip() for line in batch_lines]
            logging.info(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—Ç–≤–µ—Ç–æ–≤ –≤ —Ç–µ–∫—É—â–µ–π –ø–∞—Ä—Ç–∏–∏: {len(batch_answers)}")

            if len(batch_answers) > 0:
                batch_embeddings = model.encode(batch_answers, convert_to_tensor=False)
                logging.info(f"–†–∞–∑–º–µ—Ä—ã —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —Ç–µ–∫—É—â–µ–π –ø–∞—Ä—Ç–∏–∏: {len(batch_embeddings)}")

                original_answers.extend(batch_answers)
                all_embeddings.append(batch_embeddings)
            else:
                logging.warning(f"–ü—É—Å—Ç–∞—è –ø–∞—Ä—Ç–∏—è –ø—Ä–∏ –∏–Ω–¥–µ–∫—Å–∞—Ö {i} - {i + batch_size}")

        if all_embeddings:
            all_embeddings = np.vstack(all_embeddings)
            logging.info(f"–û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—Ç–≤–µ—Ç–æ–≤: {len(original_answers)}")
            logging.info(f"–†–∞–∑–º–µ—Ä—ã –≤—Å–µ—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {all_embeddings.shape}")
        else:
            logging.error("–û—à–∏–±–∫–∞: –Ω–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–∏.")

        return original_answers, all_embeddings
    except Exception as e:
        logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ –¥–∞–Ω–Ω—ã—Ö –∏–ª–∏ –≤—ã—á–∏—Å–ª–µ–Ω–∏–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {e}")
        return [], np.array([])

async def read_data_with_embeddings_async(file_path, model, batch_size=10):
    loop = asyncio.get_event_loop()
    original_answers, all_embeddings = await loop.run_in_executor(executor, read_data_with_embeddings, file_path, model, batch_size)

    if len(original_answers) == 0 or all_embeddings.size == 0:
        logging.error("–û—à–∏–±–∫–∞: —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –∏–ª–∏ –ø—É—Å—Ç—ã. –í read_data_with_embeddings_async")
    else:
        logging.info(f"–£—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ {len(original_answers)} –æ—Ç–≤–µ—Ç–æ–≤ –∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —Ä–∞–∑–º–µ—Ä–æ–º {all_embeddings.shape}.")

    return original_answers, all_embeddings

async def on_startup():
    global original_answers, answer_embeddings
    logging.info("on_startup –≤—ã–∑—ã–≤–∞–µ—Ç—Å—è...")
    try:
        original_answers, answer_embeddings = await read_data_with_embeddings_async('data.txt', model)
        if len(original_answers) == 0 or answer_embeddings.size == 0:
            logging.error("–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –¥–∞–Ω–Ω—ã—Ö –∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –Ω–∞ —Å—Ç–∞—Ä—Ç–µ.")
        else:
            logging.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(original_answers)} –æ—Ç–≤–µ—Ç–æ–≤ –∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —Ä–∞–∑–º–µ—Ä–æ–º {answer_embeddings.shape}.")
        return original_answers, answer_embeddings
    except Exception as e:
        logging.error(f"–û—à–∏–±–∫–∞ –≤ on_startup: {e}")
        return [], np.array([])

async def get_bot_response(user_input, response_history, original_answers, answer_embeddings):
    processed_input = await preprocess_text_spacy_async(user_input)
    if not processed_input.strip():
        return "–ò–∑–≤–∏–Ω–∏—Ç–µ, —è –Ω–µ –ø–æ–Ω–∏–º–∞—é –≤–∞—à –∑–∞–ø—Ä–æ—Å."

    try:
        user_embedding = model.encode(processed_input, convert_to_tensor=False)

        if isinstance(answer_embeddings, list):
            answer_embeddings = np.array(answer_embeddings)

        if answer_embeddings.size == 0:
            logging.error("–û—à–∏–±–∫–∞: —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –∏–ª–∏ –ø—É—Å—Ç—ã.")
            return "–û—à–∏–±–∫–∞: —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –∏–ª–∏ –ø—É—Å—Ç—ã."

        if user_embedding.shape[0] != answer_embeddings.shape[1]:
            logging.error(f"–û—à–∏–±–∫–∞: —Ä–∞–∑–º–µ—Ä—ã —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –Ω–µ —Å–æ–≤–ø–∞–¥–∞—é—Ç. user_embedding.shape: {user_embedding.shape}, answer_embeddings.shape: {answer_embeddings.shape}")
            return "–û—à–∏–±–∫–∞: —Ä–∞–∑–º–µ—Ä—ã —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –Ω–µ —Å–æ–≤–ø–∞–¥–∞—é—Ç."

        similarities = util.cos_sim(user_embedding, answer_embeddings)[0]

        sorted_indices = np.argsort(-similarities)

        best_match_score = -1
        best_match_response = None

        for idx in sorted_indices:
            response = original_answers[idx]
            if not response_history or response != response_history[-1]:
                word_matches = count_word_matches(processed_input, await preprocess_text_spacy_async(response))
                if word_matches > 0:
                    current_score = similarities[idx] + word_matches * 0.1
                    if current_score > best_match_score:
                        best_match_score = current_score
                        best_match_response = response

        if best_match_response:
            if len(best_match_response) > MAX_MESSAGE_LENGTH:
                logging.info(f"–°–æ–æ–±—â–µ–Ω–∏–µ —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω–æ–µ, –¥–ª–∏–Ω–∞: {len(best_match_response)}")
                return split_message(best_match_response)
            return best_match_response

        return "–ò–∑–≤–∏–Ω–∏—Ç–µ, —è –Ω–µ –Ω–∞—à–µ–ª –ø–æ–¥—Ö–æ–¥—è—â–µ–≥–æ –æ—Ç–≤–µ—Ç–∞. –ú–æ–∂–µ—Ç–µ –ø–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –≤–∞—à –∑–∞–ø—Ä–æ—Å."
    except Exception as e:
        logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã—á–∏—Å–ª–µ–Ω–∏–∏ –ø–æ—Ö–æ–∂–µ—Å—Ç–∏ –∏–ª–∏ –≤—ã–±–æ—Ä–µ –æ—Ç–≤–µ—Ç–∞: {e}")
        return "–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –≤–∞—à–µ–≥–æ –∑–∞–ø—Ä–æ—Å–∞. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –µ—â–µ —Ä–∞–∑."

@router.callback_query(F.data == 'chat_gpt')
async def get_chat_gpt(callback: CallbackQuery, state: FSMContext):
    kb_back = InlineKeyboardBuilder()  # –°–æ–∑–¥–∞–Ω–∏–µ –∫–ª–∞–≤–∏–∞—Ç—É—Ä—ã
    kb_back.row(InlineKeyboardButton(text='‚¨ÖÔ∏è –ù–∞–∑–∞–¥', callback_data='to_main')) # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∫–Ω–æ–ø–∫–∏ "–Ω–∞–∑–∞–¥"
    await state.set_state(ChatGPT.Set_Chat_GPT)
    await callback.message.edit_text(text='<b>üåü –î–æ–±—Ä–æ –ø–æ–∂–∞–ª–æ–≤–∞—Ç—å! –ó–¥–µ—Å—å –≤—ã –º–æ–∂–µ—Ç–µ –ø–æ–æ–±—â–∞—Ç—å—Å—è —Å AI-–ø–æ–º–æ—â–Ω–∏–∫–æ–º –ú–∞–∫—Å–∏–º–æ–º.\n\n'
                                          'üìö –ù–∞ –¥–∞–Ω–Ω—ã–π –º–æ–º–µ–Ω—Ç –ú–∞–∫—Å–∏–º –∑–Ω–∞–µ—Ç –≤—Å–µ –æ —Ä–µ–≥–ª–∞–º–µ–Ω—Ç–µ –ø—Ä–æ–µ–∫—Ç–∞ –∏ –º–æ–∂–µ—Ç –¥–∞—Ç—å —Ç–æ—á–Ω—ã–π –æ—Ç–≤–µ—Ç –Ω–∞ –≤–∞—à –≤–æ–ø—Ä–æ—Å!\n\n'
                                          '‚ùóÔ∏è –ú–∞–∫—Å–∏–º –¥–∞–µ—Ç —Ç–æ—á–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é —Ç–æ–ª—å–∫–æ —Ç–æ–≥–¥–∞, –∫–æ–≥–¥–∞ –≤—ã —è—Å–Ω–æ –∏–∑–ª–æ–∂–∏–ª–∏ —Å–≤–æ—é –º—ã—Å–ª—å!\n\n'
                                          'üèì –î–ª—è –≤—ã—Ö–æ–¥–∞ –∏–∑ –¥–∏–∞–ª–æ–≥–∞ –Ω–∞–ø–∏—à–∏—Ç–µ –≤ —á–∞—Ç —Å–ª–æ–≤–æ "exit".\n\n'
                                          '‚ö°Ô∏è –ë—É–¥—É—â–µ–µ —É–∂–µ –∑–¥–µ—Å—å - –Ω–∞ BLACK RUSSIA!</b>', parse_mode='HTML', reply_markup=kb_back.as_markup())
    await callback.answer()

@router.message(ChatGPT.Set_Chat_GPT)
async def start_gpt(message: Message, state: FSMContext):
    try:
        if message.text.lower() == 'exit':
            kb_back = InlineKeyboardBuilder()  # –°–æ–∑–¥–∞–Ω–∏–µ –∫–ª–∞–≤–∏–∞—Ç—É—Ä—ã
            kb_back.row(InlineKeyboardButton(text='‚¨ÖÔ∏è –ù–∞–∑–∞–¥', callback_data='to_main'))  # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∫–Ω–æ–ø–∫–∏ "–Ω–∞–∑–∞–¥"
            await message.reply("<b>–ë—ã–ª–æ –ø—Ä–∏—è—Ç–Ω–æ —Å –≤–∞–º–∏ –ø–æ–æ–±—â–∞—Ç—å—Å—è. –î–æ –Ω–æ–≤—ã—Ö –≤—Å—Ç—Ä–µ—á!</b>", parse_mode='HTML', reply_markup=kb_back.as_markup())
            await state.clear()
            return

        response_history = []

        try:
            global original_answers, answer_embeddings
            response = await get_bot_response(message.text, response_history, original_answers, answer_embeddings)
            response_history.append(response)
            for part in split_message(response):
                await message.reply(f'<b>{part}</b>', parse_mode='HTML')
        except Exception as e:
            await message.reply(f"–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞: {e}")
    except Exception as e:
        logging.error(f'–û—à–∏–±–∫–∞ –≤ –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–µ —Å–æ–æ–±—â–µ–Ω–∏–π: {e}')